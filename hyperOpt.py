# -*- coding: utf-8 -*-
"""RealWorldData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T7HKdyV5JfU8XD1GnhmwODUwYFgsAuIK
"""


# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import csv
import numpy as np
import tensorflow as tf
import pandas as pd
import os
import datetime
from tensorflow.keras import Sequential
from tensorflow.keras.layers import LSTM,Dense,AveragePooling1D,Flatten,TimeDistributed,Dropout

from bayes_opt import BayesianOptimization
from bayes_opt import UtilityFunction

#paramterbounds
hyperbounds={'window_size':(50,500), # possible[25,50,100,125,200,250,500] #discrete
'batch_size' :(5,100),    #integer
'hiddenlayer1' : (50,500), #integer
'hiddenlayer2' :(50,500), #integer
'hiddenlayer3' : (50,500), #integer
'dropout1' :(0,0.5),
'dropout2':(0,0.5),
'dropout3':(0,0.5),
'epochs':(20,100)} #integer
#HyperParameters Dictionary
hyperparameters={'window_size':200,
'batch_size' :10,
'hiddenlayer1' : 250,
'hiddenlayer2' :100,
'hiddenlayer3' : 50,
'dropout1' :0.5,
'dropout2':0.5,
'dropout3':0.5,
'epochs':20}


optimizer = BayesianOptimization(
    f=None,
    pbounds=hyperbounds,
    verbose=2,
    random_state=1,
)
  
utility = UtilityFunction(kind="ucb", kappa=2.5, xi=0.0)

next_point_to_probe = optimizer.suggest(utility)


def discretize(hyperparamters):
  disc1 =hyperparamters['window_size']
  bounds= [500,250,200,125,100,50,25]
  for bound in bounds:
    if(disc1>bound):
      hyperparamters['window_size']=bound
      break
  hyperparamters['batch_size'] = int(hyperparamters['batch_size'])
  hyperparamters['hiddenlayer1'] = int(hyperparamters['hiddenlayer1'])
  hyperparamters['hiddenlayer2'] = int(hyperparamters['hiddenlayer2'])
  hyperparamters['hiddenlayer3'] = int(hyperparamters['hiddenlayer3'])
  hyperparamters['epochs'] = int(hyperparamters['epochs'])
  return hyperparamters
optimizer.register(
    params=next_point_to_probe,
    target=1.0,
)
print(discretize(next_point_to_probe))

activities = ['climbingdown', #1
 'climbingup',#2
 'jumping',#3
 'lying',#4
 'running',#5
 'sitting',#6
 'standing',#7
 'walking']#8
users =['proband1',
        'proband3',
        #'proband5',
        #'proband9',
        #'proband10',
        #'proband11',
        #'proband12',
        #'proband15'
        ]
sensor_position = ['chest','forearm','head','shin','thigh','upperarm','waist']
complete_data=[]
complete_label=[]

 

def get_user_data(user):
  for i,activity in enumerate(activities):
    #print(activity)
    fulldata =[]
    minsize=999999
    
    for pos in sensor_position:
      with open("RealWorldData/"+user+"/data/acc_"+activity+"_csv/acc_"+activity+"_"+pos+".csv") as csvfile:
        accdata =  np.genfromtxt(csvfile, delimiter=',')
        fulldata.append(accdata/10.0)
        #print(accdata.shape)
        if(accdata.shape[0]<minsize):
          minsize=accdata.shape[0]
      
      with open("RealWorldData/"+user+"/data/gyr_"+activity+"_csv/Gyroscope_"+activity+"_"+pos+".csv") as csvfile:
        gyrdata =  np.genfromtxt(csvfile, delimiter=',')
        fulldata.append(gyrdata/10.0) 
        if(gyrdata.shape[0]<minsize):
          minsize=gyrdata.shape[0]
    minsize=int(minsize/1000)*1000
    #print(minsize)
    full_data = [ x[1:minsize+1,2:5] for x in fulldata ]
    full_data =np.column_stack(tuple(full_data))
    full_label = [i+1]
    #batched_label=[i+1]*int(minsize/windowsize)
    #batched_data =np.split(full_data,minsize/windowsize)
    complete_data.append(full_data)
    complete_label.append(full_label)
    #full_data =np.column_stack((fulldata[0][1:minsize,2:5],fulldata[1][1:minsize,2:5],fulldata[2][1:minsize,2:5],fulldata[3][1:minsize,2:5],fulldata[4][1:minsize,2:5],fulldata[5][1:minsize,2:5],fulldata[6][1:minsize,2:5]))
  return complete_data,complete_label

complete_data=[]
complete_label=[]
for user in users:
  print(user)
  data,label=get_user_data(user)

print(np.shape(complete_data))

def input_windowing(complete_data,complete_label,windowsize):
  data=[]
  label=[]
  for activity,lbl in zip(complete_data,complete_label):
    size=activity.shape[0]
    batched_label=[lbl]*int(size/windowsize) 
    batched_data =np.split(activity,size/windowsize)
    label.extend(batched_label)
    data.extend(batched_data)
  return data,label

def model_optimization(complete_data,complete_label,hyperparamters):
  win_data,win_label = input_windowing(complete_data,complete_label,hyperparamters['window_size'])
  complete_data = np.array(win_data)
  complete_label = np.array(win_label)
  size=complete_data.shape[0]
  complete_ds=tf.data.Dataset.from_tensor_slices((complete_data,complete_label)).shuffle(size)
  train_size = int(0.7 * size)
  train_ds = complete_ds.take(train_size).batch(hyperparamters['batch_size'])
  val_ds = complete_ds.skip(train_size).batch(hyperparamters['batch_size'])
  model = Sequential()
  model.add(LSTM(hyperparamters['hiddenlayer1'], input_shape=(hyperparamters['window_size'], 42),return_sequences=True))
  model.add(Dropout(hyperparamters['dropout1']))
  model.add(LSTM(hyperparamters['hiddenlayer2']))
  model.add(Dropout(hyperparamters['dropout2']))
  model.add(Dense(hyperparamters['hiddenlayer3'], activation='relu'))
  model.add(Dropout(hyperparamters['dropout3']))
  model.add(Dense(9, activation='softmax'))
  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  print("model created")
  model.fit(train_ds, epochs=hyperparamters['epochs'], verbose=1,validation_data=val_ds,use_multiprocessing=True)
  return sum(model.history.history['val_accuracy'][-10:])/10 #return average of last 10 val accuracy

for _ in range(5):
    next_point = optimizer.suggest(utility)
    print(discretize(next_point))
    target = model_optimization(complete_data,complete_label,discretize(next_point))
    optimizer.register(params=next_point, target=target)
    
    print(target)
print(optimizer.max)
